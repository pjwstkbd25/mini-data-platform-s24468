version: "3.8"

services:
  ###############################################################################
  #  DATABASE (PostgreSQL 15 with logical replication enabled)
  ###############################################################################
  db:
    image: debezium/postgres:15
    container_name: db
    ports:
      - "5433:5432"
    environment:
      POSTGRES_USER: Jarek
      POSTGRES_PASSWORD: Jarek
      POSTGRES_DB: data
      POSTGRES_HOST_AUTH_METHOD: md5
    command: postgres -c wal_level=logical -c max_replication_slots=10 -c max_wal_senders=10
    volumes:
      - pgdata:/var/lib/postgresql/data
    restart: unless-stopped

  ###############################################################################
  #  ZOOKEEPER (required by cp-kafka 7.5.0)
  ###############################################################################
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    healthcheck:
      test: [ "CMD", "bash", "-c", "echo > /dev/tcp/localhost/2181" ]
      interval: 10s
      timeout: 5s
      retries: 12
    restart: unless-stopped

  ###############################################################################
  #  KAFKA BROKER — single node, expose 29092 for host access
  ###############################################################################
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper             # simple depends_on (ZK container must start first)
    ports:
      - "9092:9092"           # for services within the Docker network
      - "29092:29092"         # for clients on the host (outside Docker)
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      # Two listeners: one for internal Docker network, one for host
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    restart: unless-stopped

  ###############################################################################
  #  SCHEMA REGISTRY (for AVRO support)
  ###############################################################################
  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    container_name: schema-registry
    depends_on:
      - kafka
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:9092
    ports:
      - "8081:8081"
    restart: unless-stopped

  ###############################################################################
  #  KAFKA CONNECT (Debezium Connect with Avro converters)
  ###############################################################################
  connect:
    build: # build custom image with Debezium plugin
      context: .
      dockerfile: Dockerfile.connect
    container_name: connect
    depends_on:
      - schema-registry
      - kafka
      - db
    environment:
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      # ---- internal topics ----
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_CONFIG_STORAGE_PARTITIONS: 1
      CONNECT_OFFSET_STORAGE_PARTITIONS: 1
      CONNECT_STATUS_STORAGE_PARTITIONS: 1
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_GROUP_ID: "1"
      CONNECT_CONFIG_STORAGE_TOPIC: debezium_configs
      CONNECT_OFFSET_STORAGE_TOPIC: debezium_offsets
      CONNECT_STATUS_STORAGE_TOPIC: debezium_status
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONNECT_PLUGIN_PATH: "/usr/share/confluent-hub-components"
    ports:
      - "8083:8083"
    restart: unless-stopped

  ###############################################################################
  #  KAFDROP — web GUI for viewing Kafka topics (with Schema Registry integration)
  ###############################################################################
  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    container_name: kafdrop
    depends_on:
      - kafka
    environment:
      KAFKA_BROKER_CONNECT: kafka:9092
      SCHEMAREGISTRY_CONNECT: http://schema-registry:8081
    ports:
      - "9000:9000"
    restart: unless-stopped

  ###############################################################################
  #  PYTHON APPLICATION (Flask app + CSV loader)
  ###############################################################################
  web:
    build: .
    container_name: web
    command: |
      bash -c "python main.py && flask run --host=0.0.0.0"
    environment:
      FLASK_APP: app.py
      FLASK_ENV: development
    ports:
      - "5000:5000"
    depends_on:
      - db
    restart: unless-stopped

  ###############################################################################
  #  CONNECTOR INIT SCRIPT (register Debezium connector after Connect starts)
  ###############################################################################
  connector-init:
    image: curlimages/curl:latest
    container_name: connector_init
    volumes:
      - ./init-connector.sh:/init-connector.sh        # shell script to register connector
      - ./init-connector.json:/init-connector.json    # connector configuration in JSON
    entrypoint: [ "sh", "/init-connector.sh" ]
    depends_on:
      - connect
      - db
      - kafka
    restart: "no"
###############################################################################
#  APACHE SPARK 3.5  – pojedynczy master uruchomiony w tle
###############################################################################
  spark:
    image: bitnami/spark:3.5.0        # lekka, gotowa do pracy
    container_name: spark
    depends_on:
      - kafka                         # Spark → Kafka
    environment:
      - SPARK_MODE=master             # ← to trzyma proces przy życiu
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_UI_PORT=4040            # UI dostępne na hoście
    ports:
      - "4040:4040"                   # http://localhost:4040
    volumes:
      - ./spark-jobs:/opt/spark-jobs  # nasze skrypty wewnątrz kontenera
    restart: unless-stopped
volumes:
  pgdata:
