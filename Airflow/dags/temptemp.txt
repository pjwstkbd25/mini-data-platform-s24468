import os
import time
from pathlib import Path
from typing import List

import pandas as pd
from kaggle.api.kaggle_api_extended import KaggleApi
from sqlalchemy import create_engine, Integer, Float, Text, DateTime, Boolean, text
from dotenv import load_dotenv


# -------------------- Env / Paths --------------------
# BASE_DIR -> folder "Airflow"
BASE_DIR = Path(__file__).resolve().parents[1]
ENV_PATH = BASE_DIR / ".env"
load_dotenv(dotenv_path=ENV_PATH)

def resolve_env_path(var_name: str, default_subpath: str) -> str:
    raw = os.getenv(var_name)
    if raw:
        p = Path(raw)
        if not p.is_absolute():
            p = BASE_DIR / p
    else:
        p = BASE_DIR / default_subpath
    p = p.resolve()
    os.environ[var_name] = str(p)
    return str(p)

# Ujednolić ścieżki z .env (względne -> absolutne)
resolve_env_path("KAGGLE_CONFIG_DIR", "Airflow/secrets")
resolve_env_path("KAGGLE_DEST_DIR", "Airflow/data/datasets")


# -------------------- Helpers --------------------
def env(name: str, default: str | None = None, required: bool = False) -> str:
    val = os.getenv(name, default)
    if required and (val is None or val == ""):
        raise RuntimeError(f"Missing required environment variable: {name}")
    return val


# def get_engine():
#     user = env("PG_USER", required=True)
#     password = env("PG_PASSWORD", required=True)
#     host = env("PG_HOST", "db")
#     port = int(env("PG_PORT", "5432"))
#     database = env("PG_DB", required=True)
#     schema = env("PG_SCHEMA", None)
#
#     engine = create_engine(f"postgresql://{user}:{password}@{host}:{port}/{database}", pool_pre_ping=True)
#     if schema:
#         with engine.connect() as conn:
#             conn.execute(text(f"CREATE SCHEMA IF NOT EXISTS {schema};"))
#             conn.execute(text(f"SET search_path TO {schema};"))
#     return engine


def infer_sqlalchemy_type(series: pd.Series):
    if pd.api.types.is_bool_dtype(series):
        return Boolean()
    if pd.api.types.is_integer_dtype(series):
        return Integer()
    if pd.api.types.is_float_dtype(series):
        return Float()
    if pd.api.types.is_datetime64_any_dtype(series):
        return DateTime()
    return Text()


# -------------------- Kaggle download --------------------
def download_kaggle_dataset(dataset_slug: str, files: List[str] | None, dest_dir: str) -> List[Path]:
    """
    Pobiera cały dataset lub wybrane pliki i rozpakowuje do dest_dir.
    Zwraca listę ścieżek do plików CSV.
    """
    dest = Path(dest_dir)
    dest.mkdir(parents=True, exist_ok=True)

    # Walidacja obecności kaggle.json
    cfg_dir = Path(env("KAGGLE_CONFIG_DIR", required=True))
    if not (cfg_dir / "kaggle.json").exists():
        raise FileNotFoundError(f"Brak pliku kaggle.json w {cfg_dir}")

    api = KaggleApi()
    api.authenticate()

    if files:
        for f in files:
            api.dataset_download_file(dataset=dataset_slug, file_name=f, path=dest, force=True, quiet=False)
    else:
        api.dataset_download_files(dataset=dataset_slug, path=dest, unzip=True, force=True, quiet=False)

    # Rozpakuj ewentualne pojedyncze ZIP-y z dataset_download_file
    for z in dest.glob("*.zip"):
        import zipfile
        with zipfile.ZipFile(z, "r") as zip_ref:
            zip_ref.extractall(dest)
        z.unlink(missing_ok=True)

    csvs = list(dest.rglob("*.csv"))
    if not csvs:
        raise RuntimeError(f"Brak plików CSV po pobraniu datasetu {dataset_slug} do {dest}")
    return csvs


# -------------------- Load to Postgres (na później) --------------------
def wait_for_db(engine, seconds: int = 60):
    for _ in range(seconds):
        try:
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            return
        except Exception:
            time.sleep(1)
    raise RuntimeError("PostgreSQL not available after waiting.")


def load_csv_to_postgres(csv_path: Path, table_name: str, engine):
    df = pd.read_csv(csv_path)

    # auto cast prostych dat/czasów po nazwie kolumn (opcjonalnie)
    for col in df.columns:
        name = col.lower()
        if any(name.endswith(suf) for suf in ("_date", "_dt", "_timestamp")):
            df[col] = pd.to_datetime(df[col], errors="ignore")

    dtype_map = {col: infer_sqlalchemy_type(df[col]) for col in df.columns}

    df.to_sql(name=table_name, con=engine, if_exists="replace", index=False, dtype=dtype_map)

    with engine.connect() as conn:
        conn.execute(text(f"""
            DO $$
            BEGIN
                IF NOT EXISTS (
                    SELECT 1 FROM information_schema.columns
                    WHERE table_name = '{table_name}' AND column_name = 'id'
                ) THEN
                    ALTER TABLE {table_name} ADD COLUMN id SERIAL PRIMARY KEY;
                END IF;
            END $$;
        """))


# -------------------- Orchestrator --------------------
def run_datagen():
    dataset_slug = env("KAGGLE_DATASET", required=True)  # np. "sumansharmadataworld/depression-surveydataset-for-analysis"
    files_env = env("KAGGLE_FILES", "")                   # np. "file1.csv,file2.csv"
    dest_dir = env("KAGGLE_DEST_DIR", str(BASE_DIR / "data" / "datasets"))
    # table_prefix = env("PG_TABLE_PREFIX", "kaggle_")    # na później

    files = [f.strip() for f in files_env.split(",") if f.strip()] or None

    csv_files = download_kaggle_dataset(dataset_slug, files, dest_dir)

    # Testowy output – tylko wypisz ścieżki CSV (bez DB)
    print([str(p) for p in csv_files])

    # --- Sekcja DB do użycia później ---
    # engine = get_engine()
    # wait_for_db(engine)
    # for csv in csv_files:
    #     base = csv.stem.lower().replace("-", "_").replace(" ", "_")
    #     table = f"{table_prefix}{base}"
    #     load_csv_to_postgres(csv, table, engine)
    #     print(f"[OK] Loaded {csv.name} -> table {table}")


if __name__ == "__main__":
    run_datagen()
